## MongoDB分页方案

## 方案列表

* 1）数据量小，不做任何处理
  * 显示出所有页，可以直接跳转到任意一页，但是数据量小，只有几十页。

* 2）只能上下翻
  * 只有上一页，下一页，但是可以一直翻页。
* 3）限制翻页范围
  * 不能让用户一次进行20页以上的翻页操作。比如第一页最多只能直接翻到二十页，同样也可以一直翻页。
* 4）空间换时间
  * 缓存中存储 page及其对应的 id，通过id来实现任意的跳页。



## 对应的实现

### 方案一--数据量小

显示出所有页，可以直接跳转到任意一页，但是数据量小，只有几十页。

这种情况直接使用`skip`、`limit`翻页即可。数据量小不需要考虑太多。

```javascript
db.test.find().sort(date:-1).skip(page*size).limit(size);
```



### 方案二-只能上下翻页

只能翻到上一页，下一页，但是可以一直翻页。

```javascript
db.test.find({“date”:{“$lt”1352194000:}}).sort(date:-1).limit(size)
```

这样每次查询时，带着上一页的最后一个或者第一个参数直接作为条件过滤。

这样计算翻到1W页也只会查询20条数据，效率很高。



### 方案三-限制跳页范围

不能让用户一次进行20页以上的翻页操作。比如第一页最多只能返回二十页，同样也可以一直翻页。

> 方案三就是方案二的一个优化，让用户可以在一定范围内进行跳页。
>
> 假设每页 20 条数据,将 10页 合并为一组，这样一组 200 条数据，使用 skip 来分页也很快。



```javascript
db.test.find({“date”:{“$lt”1352194000:}}).sort(date:-1).skip(page*size).limit(size);
```

这样1~20页都根据第一页第一条数据来进行过滤，到21页的时候就根据21页第一条数据来过滤，一直到40页。

> 做正常查询逻辑的时候把临界值的过滤条件查询出来备用。

一般也没必要做太多，正常情况下用户最多也就翻个几十页。不可能翻到几万页去。

> 带条件，一次性跳转几万页这种操作只能使用后面的空间换时间的操作来实现了。



其他需求：首页、尾页

排序换一下即可，升序换成降序。

```javascript
db.test.find({“date”:{“$lt”1352194000:}}).sort(date:-1).limit(size);

db.test.find({“date”:{“$lt”1352194000:}}).sort(date:1).limit(size);
```

### 方案四-空间换时间

缓存关键索引信息。主要是用户用于排序的字段

以ID为唯一索引的表为例，用户查询后也是按照ID进行排序的，不提供其他排序方式，那么可以采用每10个ID缓存一次的数据结构进行存储，采取10的原因是一般每页显示数量为10,20或50比较常见。如果采用的是键值对方式的缓存方案，如MemberCached或Redis，可以存储为如下结构：

key 为page，value为指定页的起始id。

| key  | value  |
| ---- | ------ |
| 1    | 000001 |
| 2    | 000011 |
| 3    | 000021 |
| 4    | 000031 |
| ……   | ……     |
| 100W | 999999 |

比如用户直接返回100W页，那么去缓存中查询到对应id为999999.

则通过如下查询即可快速查到对应结果。

```javascript
db.test.find({“id”:{“$lt”999999:}}).sort(date:-1).limit(size)
```

时间：缓存响应时间（常数）+数据库响应时间（常数）。

空间：设ID为64位，数据量在200亿以内，则需缓存n * (64 + 32) / 10位大小，大约为n * 1.2 Byte。一个亿也就是120MB的占用。

## 例子

这里就是几百万数据还要随意跳页的一个需求，优化前翻页特别慢。

具体查询数据如下：

```javascript
  "keysExamined" : 5740080,
  "docsExamined" : 5740080,    
   "storage" : {
        "data" : {
            "bytesRead" : NumberLong(8705720677),
            "timeReadingMicros" : NumberLong(70347680)
        },
        "timeWaitingMicros" : {
            "cache" : NumberLong(17)
        }
    },
    "responseLength" : 139,
    "protocol" : "op_msg",
    "millis" : 110063,
```

通过 skip 来分页，简直惨不忍睹。

Profiling 显示索引查询了 574W 记录，文档也扫了 574W条，读取数据 8.7G,读取数据就耗时 70秒，整个查询耗时 110秒。



优化过程

这是一个 skip 分页的查询，直接翻到了最后一页.

```javascript
        "limit" : NumberLong(50), 
        "skip" : NumberLong(5739400), 
```

> skip 了几百万行数据，能不慢吗。



由于需要直接跳到任意页面，于是方案二三也用不了了。

只能使用方案四，将id存到redis里，每次查询先去找到对应id来过滤。

