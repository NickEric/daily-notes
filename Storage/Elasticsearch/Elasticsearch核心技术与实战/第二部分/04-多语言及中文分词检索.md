# 多语言及中文分词检索

## 1. 场景

### 1. 自然语言与查询 Recall

* 当处理自然语言时，有些情况，尽管搜索和原文不完全匹配，但还是希望搜索到一些内容。
  * 比如搜 Jumping fox 时也希望能搜到 Jumped foxes
* 一些可采取的优化
  * 归一化词元 - 清除变音符号
  * 抽取词根 - 清除单复数和时态差异
  * 包含同义词
  * 拼写错误 - 拼写错误，或者同音异形词

### 2. 混合多语言

* 具体场景
  * 不同的索引使用不同的语言
  * 同索引中，不同字段使用不同语言
  * 同一字段内使用不同语言
* 挑战
  * 词干提取 - 以色列文档中，可能会包含希伯来语、阿拉伯语、俄语和英语等
  * 不正确的文档频率 - 单词出现频率越低权重则会越高
  * 需要判定用户搜索时使用的语言 - 语言识别
    * 例如 根据语言去查询不同的索引



### 3. 分词

* 英文分词
  * You're 分成一个还是多个
* 中文分词
  * 分词标准 - 不同标准中分词结果不一致
  * 歧义 - 中文天生存在歧义



### 4. 中文分词方法演变

**字典法**

* 查字典 - 最容易想到的分词方法（北京航空大学梁南元教授提出）
  * 一个句子从左到右扫一遍。遇到有的词就标示出来。找到复合词，就找最长的
  * 不认识的字串就分割成单个字
* 最小词数的分词理论 - 哈工大王晓龙博士把查字典的方法理论化
  * 一句话应该分成数量最少的词串
  * 遇到二义性的分割，无能为力
    * 例如 发展中国家-分为 发展中 还是 中国呢
  * 用各种文化规则来解决二义性，都并不成功



**基于统计法的机器学习算法**

* 统计语言模型 - 1990年前后，清华大学电子工程系郭进博士
  * 解决了二义性问题，将中文分词的错误率降低了一个数量级。
  * 概率问题，动态规划 + 利用维特比算法快速找到最佳分词
* 基于统计的机器学习算法
  * 目前常用的是 HMM、CRF、SVM、深度学习等算法、
  * 随着深度学习的兴起，也出现了基于神经网络的分词器。



**中文分词现状**

* 经过几十年发展，今天基本可以看作是一个已经解决的问题
* 不同分词器的好坏，主要的差别在于数据的使用和工程使用的精度
* 常见分词器都是使用机器学习算法和词典相结合，一方面提高准确率，另一方面能够改善领域适用性。 



**中文分词器**

* HanLP - 面向生产环境的自然语言处理工具包
  * `https://github.com/KennFalcon/elasticsearch-analysis-hanlp`
  * `www.hanlp.com`
* IK 分词器
  * `https://github.com/medcl/elasticsearch-analysis-ik`
* 拼音分词器
  * `https://github.com/medcl/elasticsearch-analysis-pinyin`
  * 只需要输入拼音就能查询到结果 就是用的 拼音分词器

插件安装

```shell
# syntax ./elasticsearch-plugin install url
./elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.8.0/elasticsearch-analysis-ik-7.8.0.zip
```

插件安装其实就是下载 zip 包然后解压到 plugins 目录下。

Docker 安装的话可以通过 Volume 的方式放在宿主机，或者进入容器用命令行安装也是一样的。

> 命令行安装的 IK 分词器，如果有 config 目录会移动到 elasticsearch 的config 中，新目录名和分词器名一致。如 /usr/share/elasticsearch/config/analysis-ik