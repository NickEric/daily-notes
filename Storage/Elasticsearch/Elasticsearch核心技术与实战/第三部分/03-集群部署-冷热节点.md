## 1. 集群部署

### 1. 节点类型

* 不同角色的节点
  * Master eligible / Data / Ingest /Coordiunating / Machine Learing
* 在开发环境，一个节点可承担多种角色
* 在生产环境中
  * 根据数据量。写入和查询的吞吐量，选择合适的部署方式
  * 建议设置单一角色的节点

一个节点在默认情况下会同时扮演: Master eligibel node、data node和 ingest node。

节点角色及其具体配置如下：

* Master Node
  * `node.mster:true`
  * `node.ingest:false`
  * `node.data:false`
* Ingest node
  * `node.mster:false`
  * `node.ingest:true`
  * `node.data:false`
* Data node
  * `node.mster:false`
  * `node.ingest:false`
  * `node.data:true`
* Coordinate node
  * `node.mster:false`
  * `node.ingest:false`
  * `node.data:false`

角色分离后，可以为不同的机器选择不同的配置。

* Master node
  * 只负责集群状态
  * 低配 CPU、RAM和磁盘
* Data node 
  * 负责数据存储和处理客户端请求 
  * 高配 CPU、RAM和磁盘
* Ingest nodes
  * 负责数据处理
  * 高配 CPU，中配 RAM,低配磁盘



Coordinate node  -- 大集群推荐配置

* 负责请求转发（Load Balance 作用，降低 Master 和 Data nodes 的负载
* 搜索结果得到 Gather、Reduce 
  * 有时候无法预知客户端会发生怎么样的请求，所以需要配置大内存，防止一个深度聚合引发 OOM
* 高或中的 CPU，RAM，磁盘则可以用低配

Master Node

* 高可用&避免脑裂的角度出发
  * 一般在生产环境只配置 3 台
  * 一个集群只有一个活跃的主节点
    * 负载分片管理、索引创建、集群管理等操作



扩展：增加节点，水平扩展

* 磁盘不够用，或磁盘压力大时：增加数据节点
* 有大量复杂查询及聚合时：增加 Coordinating 节点，提升查询性能
* 读写分离



**Kibana 部署**

官方推荐将 Kibana 部署在  Coordinating 节点上，实现 Kibana的高可用



**异地多活**

集群部署在 多 个数据中心，数据 多 写的，GTM分发读请求。



## 2. Host & Warm Architecture

### 1. Hot & Warm Architecture

* 数据通常不会有 Update 操作；适用于 Time Based  索引数据（生命周期管理），同时数据量比较大的场景。
* 引入 Warm节点，低配置大容量的机器存放老数据，以降低部署成本。
* Hot 节点：索引有不断新文档写入，通常使用 SSD
* Warm 节点：索引不存在数据写入，同时也不存在大量查询，通常使用 HDD



* **Hot Nodes** - 用于数据的写入
  * Indexing 对 CPU 和 IO 都有很高的要求。需要使用高配置机器。
  * 存储的性能要好，通常使用 SSD。
* **Warm Nodes** - 用于保存只读的索引，比较旧的数据
  * 通常使用大容量的磁盘（一般是 Spinning Disks）



### 2. 配置Hot & Warm Architecture

使用 Shard Filtering，分为以下几步：

* 1）标记节点（Tagging）
* 2）配置索引到 Hot Node
* 3）配置索引到 Warm Node



**1）标记节点（Tagging）**

需要通过`node.attr`来标记一个节点

* 节点的 attribute 可以是任意的 key/value
* 可以通过 elasticsearch.yml或者使用 -E 命令指定



```yml
# my_node_type 为 key,   hot/warm 为value都可以任意指定
node.attr.my_node_type=hot
node.attr.my_node_type=warm
```

重启节点后通过以下命令查看

```shell
GET /_cat、nodeattrs?v
```

**2）配置索引到 Hot Node**

在创建索引的时候，指定将其创建在 hot 节点上

index settings 中加入以下参数

```shell
# 其中 my_node_type 就是前面指定的 key
"index.routing.allocation.require.my_node_type"="hot"
```

例如

```shell
PUT logs_2020_07_14
{
	"sttings":{
		"index.routing.allocation.require.my_node_type"="hot"
	}
}
```

使用cat 命令查询具体分片情况

```shell
GET _cat/shards?v
```



将旧数据移动到 Warm 节点

```shell
PUT logs_2020_07_14
{
	"sttings":{
		"index.routing.allocation.require.my_node_type"="warm"
	}
}
```



### 3. Rack Awareness

数据中心服务器都是放在机架上的，如果主分片和副本分片的几台机器都在一个机架上，那么该机架突然断电就可能会丢失数据。所以需要分配在不同的机架上。

这个和 hot warm节点类似。

首先标记节点,增加`node.attr`参数

```yaml
node.attr.my_rack_id=rack1
```

修改索引的 settings

```shell
# 注意 这里用的 不是value 而是 参数名
PUT logs_2020_07_14
{
	"sttings":{
		"index.routing.allocation.awareness.attributes"="my_rack_id"
	}
}
```

es 只会尽量保证分配到不同的机架，如果就只有一个机架还是会分配成功。

这时可以添加另一个选项，遇到这样的情况就不让分配

```shell
# 这样 只有 rack1 或 只有 rack2 的情况下，副本分片是不会被分配的 只会分配主分片
PUT logs_2020_07_14
{
	"sttings":{
		"index.routing.allocation.awareness.force.zone.values"="rack1,rack2"
	}
}
```

