# ES 读写性能优化

## 1. 写入性能优化

* 写性能优化的目标：增大写吞吐量，越高越好
* 客户端：多线程，批量写
  * 可以通过性能测试，确定最佳文档数量
  * 多线程：需要观察是否有 HTTP 429 返回，实现 Retry 以及线程数量的自动调节
* 服务器端：单个性能问题，往往是多个因素造成的。需要先分解问题，在单个节点上调至并结合测试，尽可能压榨硬件资源，以达到最高吞吐量
  * 使用更好的硬件。 观察 CPU 、IO Block
  * 线程切换、堆栈状况

### 1. 服务器端优化手段

* 降低 IO 操作
  * 使用 ES 自动生成文档Id
  * 一些相关的 ES 配置，如 Refresh Interval
* 降低 CPU 和存储开销
  * 较少不必要的发内存
  * 避免不需要的 doc_values
  * 文档的字段尽量保证相同的顺序，可以提高文档的压缩率
* 尽可能做到写入和分片的负载均衡，实现水平扩展
  * Shard Filter 
  * Write Load Balancer
* 调整 Bulk 线程池和队列



### 2. 文档建模



* 只需要聚合不需要搜索，Index设置成 false
* 不需要算分，Norms 设置成 false
* 不要对字符串使用默认Dynamic Mapping。字段数量过多，会对性能产生比较大的影响
* Index_optins 控制在创建倒排索引时，哪些内容可以被添加到倒排索引中。优化这些设置，一定程度可以节约 CPU
* 关闭 _source，减少 IO 操作



### 3. 针对性能的取舍

* 牺牲可靠性：将副本分片设置为0，写入完毕再调整回去
* 牺牲搜索实时性：增加 Refresh Interval 时间
* 牺牲可靠性：修改 Translog 的配置



### 4. 数据写入过程

* Refresh
  * 将文档先保存在 Index_buffer 中，**以 Refresh_interval 为时间间隔**，定期清空 buffer,生成 Segments。借助文件系统缓存的特性，先将 Segment 放在文件系统缓存中，并开放查询，以提升搜索的实时性
* Translog
  * Segment 没有写入磁盘，即便发生了宕机，重启后，数据也能恢复。**默认配置是每次请求都会落盘。**
* Flush
  * 删除旧的 translog 文件
  * 生成 Segment 并写入磁盘 / 更新 commit point 并写入磁盘。**ES 自动完成，可优化点不多**。



### 5. 例子

```shell
PUT myindex
{
  "settings": {
    "index": {
    	# refresh 间隔30s
      "refresh_interval": "30s",
      "number_of_shards": "2"
    },
    "routing": {
      "allocation": {
      # 控制分片，避免数据热点
        "total_shards_per_node": "3"
      }
    },
    # translog 异步写入 30s 一次
    "translog": {
      "sync_interval": "30s",
      "durability": "async"
    },
    "number_of_replicas": 0
  },
  "mappings": {
    "dynamic": false,
    "properties": {}
  }
}
```



## 2. 查询性能优化

* ES != 关系型税局看
* 尽可能使用 反范式数据，从而获取最佳性能
  * 使用 Nested 类型的数据，查询速度会慢几倍
  * 使用 Parent/Child 关系，查询速度会慢几百倍



### 1. 数据建模

* 尽量将数据先进行计算，然后在保存到ES，避免查询时的 Script 计算
* 尽量使用 Filter Context，利用缓存机制，较少不必要的算分
* 结合 profile，explain API 分析慢查询的问题，持续优化数据模型
  * 严禁使用 *开头通配符 Terms 查询



### 2. 优化分片

* 避免 Over Sharing
  * 一次查询需要访问每一个分片，分片过多，会导致不必要的查询开销
* 结合场景，控制单个分片的大小
  * Search 20GB
  * Loggin 40GB
* Force-merge  Read-only 索引
  * 使用基于时间序列的索引，将只读的索引进行 force merge，减少 Segment 数量