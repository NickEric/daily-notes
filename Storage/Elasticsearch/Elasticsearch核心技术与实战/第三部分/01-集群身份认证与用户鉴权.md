# 集群身份认证与用户鉴权



## 1. 概述

Elasticsearch 信息泄露问题的原因？

* Elasticsearch 在默认安装后，不提供任何形式的安全防护
* 错误的配置信息导致公网可以访问 ES 集群
  * 在 elasticsearch.yml 文件中，server.host 被错误的配置为 0.0.0.0



## 2. 数据安全性的基本需求

* 身份认证
  * 鉴定用户是否合法
* 用户鉴权
  * 指定哪个用户可以访问哪个索引
* 传输加密
* 日志审计



### 1. 一些免费方案

* 设置 Nginx 反向代理
* 安装免费的 Security 插件
* X-Pack 的 Basic 版
  * 从 ES 6.8 &7.0 开始，Security 纳入 x-pack 的 Basic 版本中，免费使用一些基本功能



### 2. Authenication -身份认证

* 认证体系的几种类型
  * 提供用户名和密码
  * 提供秘钥或 Kerberos 票据
* Realms：X-Pack 中的认证服务
  * 内置 Realms（免费）
    * File / Native（用户名密码保存在 Elasticsearch）
  * 外置 Realms（收费）
    * LDAP、Active Directory、PKI、SAML、Kerberos



### 3. 使用 X-Pack 内置 Realms

 首先需要修改配置文件，修改如下

```shell
xpack.security.enabled=true
```

开启 security 认证,接着**重启 Elasticsearch**。

然后执行以下命令设置密码

```shell
./elasticsearch-setup-passwords interactive
```

```shell
Initiating the setup of passwords for reserved users elastic,apm_system,kibana,logstash_system,beats_system,remote_monitoring_user.
You will be prompted to enter passwords as the process progresses.
Please confirm that you would like to continue [y/N]y   #y同意

#依次设置以下一个账号的密码
Enter password for [elastic]: 
Reenter password for [elastic]: 
Enter password for [apm_system]: 
Reenter password for [apm_system]: 
Enter password for [kibana]: 
Reenter password for [kibana]: 
Enter password for [logstash_system]: 
Reenter password for [logstash_system]: 
Enter password for [beats_system]: 
Reenter password for [beats_system]: 
Passwords do not match.
Try again.
Enter password for [beats_system]: 
Reenter password for [beats_system]: 
Enter password for [remote_monitoring_user]: 
Reenter password for [remote_monitoring_user]: 
Changed password for user [apm_system]
Changed password for user [kibana]
Changed password for user [logstash_system]
Changed password for user [beats_system]
Changed password for user [remote_monitoring_user]
Changed password for user [elastic]
```



然后在访问 ES 就需要输入密码了

如果一起搭了 Kibana 的，此时也需要在 Kibana 中配置账号密码，否则无法正常访问。

编辑 kibana 配置文件`/config/kibana.yml`，增加以下内容

```shell
# 账号就使用默认的 kibana 即可
elasticsearch.username: "kibana"
elasticsearch.password: "you password"
```



## 3. 集群内部通信安全

* 1）加密数据--避免数据抓包，敏感信息泄露
* 2）验证身份 - 避免 Inposter Node
  * Data / Cluster State

### 1. 为节点创建证书

* 1）TLS 
* 2）证书认证的不同级别
  * 1）Certifcate - 节点加入集群需要使用相同 CA  签发的证书
  * 2）Full Verification - 节点加入集群需要使用相同 CA  签发的证书，还需要验证 Host name 或 IP 地址
  * 3）No Verification - 任何节点都可以加入



### 2. 利用 ES 工具 创建证书

elasticsearch 提供了创建证书的工具

```shell
./bin/elasticsearch-certutil  ca
./bin/elasticsearch-certutil cert --ca elastic-stack-ca.p12
```

### 3. 配置 elasticsearch

将前面创建好的 `elastic-certificates.p12` 文件复制到elasticsearch config 目录下，推荐新建目录单独存放，然后修改配置文件`leasticsearch.yml` 增加相关配置

```yml
xpack.security.transport.ssl.enabled: true
xpack.security.transport.ssl.verification_mode: certificate 
xpack.security.transport.ssl.keystore.path: elastic-certificates.p12 
xpack.security.transport.ssl.truststore.path: elastic-certificates.p12 
```



如果使用 Full Verification 验证等级，需要修改`verification_mode`,同时在前面生成证书的时候添加相应参数`--name`, `--dns` and `--ip`。

```shell
./bin/elasticsearch-certutil cert --ca elastic-stack-ca.p12
```

官方文档`https://www.elastic.co/guide/en/elasticsearch/reference/current/configuring-tls.html#tls-transport`



### 4. 同时配置 https

同样的只需要添加配置即可，证书还是使用前面生成的那个。

```yml
xpack.security.http.ssl.enabled: true
# 证书地址
xpack.security.http.ssl.keystore.path: "http.p12"
```



### 5. 配置 kibana https 访问 es

在 kibana 配置文件中添加如下内容

```yml
elasticesarch.host:["https://<your_elasticsearch_host>:9200"]
elasticesarch.sshl.acertificateAuthorities: /path/to/your/ac.crt
```

可以看到，这里需要的是 pem 证书，文件，于是需要先使用 openssl 通过前面为 es 生成的证书生成一个 pem 证书

```shell
openssl pksc12 -in elastic-certificates.p12 -nokeys -out -elastic-ca.pem
```

最后会生成一个`elastic-ca.pem`文件，配置到前面的路径即可。



### 6. 配置 https 访问 kibana

同样的使用前面提到的 es 工具生成证书

```shell
./bin/elasticsearch-certutil ca --pem
```

会生成一个`elastic-stack-ca.zip`，解压会得到`ca.crt`和`ca.key` 两个文件

拷贝到 kibana 配置目录，在配置文件中添加如下内容

```yml
server.ssl.enable: true
server.ssl: config/certs/ca.crt
server.ssl.key: config/certs/ca.key
```



重启 kibana 的过程中，可能会有一些报错，因为证书是自签的。



## 4. 集群部署

### 1. 节点类型

* 不同角色的节点
  * Master eligible / Data / Ingest /Coordiunating / Machine Learing
* 在开发环境，一个节点可承担多种角色
* 在生产环境中
  * 根据数据量。写入和查询的吞吐量，选择合适的部署方式
  * 建议设置单一角色的节点

一个节点在默认情况下会同时扮演: Master eligibel node、data node和 ingest node。



* Master Node
  * `node.mster:true`
  * `node.ingest:false`
  * `node.data:false`
* Ingest node
  * `node.mster:false`
  * `node.ingest:true`
  * `node.data:false`
* Data node
  * `node.mster:false`
  * `node.ingest:false`
  * `node.data:true`
* Coordinate node
  * `node.mster:false`
  * `node.ingest:false`
  * `node.data:false`

角色分离后，可以为不同的机器选择不同的配置。

* Master node
  * 只负责集群状态
  * 低配 CPU、RAM和磁盘
* Data node 
  * 负责数据存储和处理客户端请求 
  * 高配 CPU、RAM和磁盘
* Ingest nodes
  * 负责数据处理
  * 高配 CPU，中配 RAM,低配磁盘



Coordinate node  -- 大集群推荐配置

* 负责请求转发（Load Balance 作用，降低 Master和Data nodes 的负载
* 搜索结果得到 Gather、Reduce 
  * 有时候无法预知客户端会发生怎么样的请求，所以需要配置大内存，防止一个深度聚合引发 OOM
* 高或中的 CPU，RAM 低 磁盘

Master Node

* 高可用&避免脑裂的角度出发
  * 一般在生产环境只配置 3 台
  * 一个集群只有一个活跃的主节点
    * 负载分片管理、索引创建、集群管理等操作



扩展：增加节点，水平扩展

* 磁盘不够用，或磁盘压力大时：增加数据节点
* 有大量复杂查询及聚合时：增加 Coordinating 节点，提升查询性能
* 读写分离



Kibana 部署

官方推荐将 Kibana 部署在  Coordinating 节点上，实现 Kibana的高可用



异地多活

集群部署在 多 个数据中心，数据 多 写的，GTM分发读请求。



## 5. Host & Warm Architecture

### 1. Hot & Warm Architecture

* 数据通常不会有 Update 操作；适用于 Time Based  索引数据（生命周期管理），同时数据量比较大的场景。
* 引入 Warm节点，低配置大容量的机器存放老数据，以降低部署成本。
* Hot 节点：索引有不断新文档写入，通常使用 SSD
* Warm 节点：索引不存在数据写入，同时也不存在大量查询，通常使用 HDD



* **Hot Nodes** - 用于数据的写入
  * Indexing 对 CPU 和 IO 都有很高的要求。需要使用高配置机器。
  * 存储的性能要好，通常使用 SSD。
* **Warm Nodes** - 用于保存只读的索引，比较旧的数据
  * 通常使用大容量的磁盘（一般是 Spinning Disks）



### 2. 配置Hot & Warm Architecture

使用 Shard Filtering，分为以下几步：

* 1）标记节点（Tagging）
* 2）配置索引到 Hot Node
* 3）配置索引到 Warm 节点



**1）标记节点（Tagging）**

需要通过`node.attr`来标记一个节点

* 节点的 attribute 可以是任意的 key/value
* 可以通过 elasticsearch.yml或者使用 -E 命令指定



```yml
# my_node_type 为 可以 value 为 hot/warm 都可以任意指定
node.attr.my_node_type=hot
node.attr.my_node_type=warm
```

重启节点后通过以下命令查看

```shell
GET /_cat、nodeattrs?v
```

**2）配置索引到 Hot Node**

在创建索引的时候，指定将其创建在 hot 节点上

index settings 中加入以下参数

```shell
# 其中 my_node_type 就是前面指定的 key
"index.routing.allocation.require.my_node_type"="hot"
```

例如

```shell
PUT logs_2020_07_14
{
	"sttings":{
		"index.routing.allocation.require.my_node_type"="hot"
	}
}
```

使用cat 命令查询具体分片情况

```shell
GET _cat/shards?v
```



将旧数据移动到 Warm 节点

```shell
PUT logs_2020_07_14
{
	"sttings":{
		"index.routing.allocation.require.my_node_type"="warm"
	}
}
```



### 3. Rack Awareness

数据中心服务器都是放在机架上的，如果主分片和副本分片的几台机器都在一个机架上，那么该机架突然断电就可能会丢失数据。所以需要分配在不同的机架上。

这个和 hot warm节点类似。

首先标记节点,增加`node.attr`参数

```yaml
node.attr.my_rack_id=rack1
```

修改索引的 settings

```shell
# 注意 这里用的 不是value 而是 参数名
PUT logs_2020_07_14
{
	"sttings":{
		"index.routing.allocation.awareness.attributes"="my_rack_id"
	}
}
```

es 只会尽量保证分配到不同的机架，如果就只有一个机架还是会分配成功。

这是可以添加另一个选项，遇到这样的情况就不让分配

```shell
# 这样 只有 rack1 或 rack2 的情况下，副本分片是不会被分配的 只会分配主分片
PUT logs_2020_07_14
{
	"sttings":{
		"index.routing.allocation.awareness.force.zone.values"="rack1,rack2"
	}
}
```

